from dataclasses import dataclass, field
from typing import Optional
import transformers

@dataclass
class ScriptArguments:
    qlora: Optional[bool] = field(default=False, metadata={"help": "whether to use QLoRA"})
    quantize: Optional[bool] = field(default=False, metadata={"help": "whether to use quantization"})
    dataset: Optional[str] = field(default="data/main_data", metadata={"help": "the location of the data"})
    beta: Optional[float] = field(default=0.1, metadata={"help": "the beta parameter for DPO loss"})
    model_name_or_path: Optional[str] = field(default="checkpoint/base", metadata={"help": "the location of the SFT model name or path"})
    max_prompt_length: Optional[int] = field(default=4096, metadata={"help": "the maximum prompt length"})
    max_length: Optional[int] = field(default=4096, metadata={"help": "the maximum sequence length"})
    sanity_check: Optional[bool] = field(default=False, metadata={"help": "only train on 1000 samples"})
    ignore_bias_buffers: Optional[bool] = field(default=False, metadata={"help": "fix for DDP issues with LM bias/mask buffers - invalid scalar type, inplace operation. See https://github.com/huggingface/transformers/issues/22482#issuecomment-1595790992"})

@dataclass
class TrainingArgumentsClass(transformers.Seq2SeqTrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    train_on_source: Optional[bool] = field(default=False, metadata={"help": "Whether to train on the input in addition to the target text."})
    full_finetune: bool = field(default=False, metadata={"help": "Finetune the entire model without adapters."})
    adam8bit: bool = field(default=False, metadata={"help": "Use 8-bit adam."})
    double_quant: bool = field(default=True, metadata={"help": "Compress the quantization statistics through double quantization."})
    quant_type: str = field(default="nf4", metadata={"help": "Quantization data type to use. Should be one of `fp4` or `nf4`."})
    bits: int = field(default=4, metadata={"help": "How many bits to use."})
    lora_r: int = field(default=8, metadata={"help": "Lora R dimension."})
    lora_alpha: float = field(default=16, metadata={"help": " Lora alpha."})
    lora_dropout: float = field(default=0.05, metadata={"help": "Lora dropout."})
    max_memory_MB: int = field(default=24000, metadata={"help": "Free memory per gpu."})
    report_to: str = field(default="none", metadata={"help": "To use wandb or something else for reporting."})
    output_dir: str = field(default="./output", metadata={"help": "The output dir for logs and checkpoints"})
    optim: str = field(default="paged_adamw_32bit", metadata={"help": "The optimizer to be used"})
    per_device_train_batch_size: int = field(default=1, metadata={"help": "The training batch size per GPU. Increase for better speed."})
    gradient_accumulation_steps: int = field(default=16, metadata={"help": "How many gradients to accumulate before to perform an optimizer step"})
    max_steps: int = field(default=10000, metadata={"help": "How many optimizer update steps to take"})
    weight_decay: float = field(default=0.0, metadata={"help": "The L2 weight decay rate of AdamW"})
    learning_rate: float = field(default=5e-4, metadata={"help": "The learning rate"})
    remove_unused_columns: bool = field(default=False, metadata={"help": "Removed unused columns. Needed to make this codebase work."})
    max_grad_norm: float = field(default=0.3, metadata={"help": "Gradient clipping max norm. This is tuned and works well for all models tested."})
    gradient_checkpointing: bool = field(default=True, metadata={"help": "Use gradient checkpointing. You want to use this."})
    do_train: bool = field(default=True, metadata={"help": "To train or not to train, that is the question?"})
    lr_scheduler_type: str = field(default="cosine", metadata={"help": "Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis, default=constant, cosine"})
    warmup_ratio: float = field(default=0.03, metadata={"help": "Fraction of steps to do a warmup for"})
    logging_steps: int = field(default=10, metadata={"help": "The frequency of update steps after which to log the loss"})
    group_by_length: bool = field(default=True, metadata={"help": "Group sequences into batches with same length. Saves memory and speeds up training considerably."})
    save_strategy: str = field(default="steps", metadata={"help": "When to save checkpoints"})
    save_steps: int = field(default=50, metadata={"help": "How often to save a model"})
    save_total_limit: int = field(default=40, metadata={"help": "How many checkpoints to save before the oldest is overwritten"})
    evaluation_strategy: Optional[str] = field(default="steps", metadata={"help": "the evaluation strategy"})
    eval_steps: Optional[int] = field(default=50, metadata={"help": "the evaluation frequency"})
    run_name: Optional[str] = field(default="dpo_llama2", metadata={"help": "the run name"})
    report_to: Optional[str] = field(default="wandb", metadata={"help": 'The list of integrations to report the results and logs to. Supported platforms are `"azure_ml"`, `"comet_ml"`, `"mlflow"`, `"neptune"`, `"tensorboard"`, `"clearml"` and `"wandb"`. Use `"all"` to report to all integrations installed, `"none"` for no integrations.'})
